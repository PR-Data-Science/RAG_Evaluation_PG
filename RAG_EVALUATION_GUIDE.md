# RAG Evaluation System â€” Quick Start Guide

## ğŸš€ Running the RAG Evaluation UI

The RAG evaluation UI provides a production-grade system for testing and debugging the Retrieval-Augmented Generation pipeline.

### Start the UI

```bash
cd /Users/pr/Downloads/Learning_Projects/LLM/Agent_UTA_HR_Policies

# Run the evaluation UI (Gradio will launch on http://127.0.0.1:7900)
./.venv/bin/python3 src/ui/rag_evaluation_ui.py
```

The UI will:
1. Run startup tests with 3 sample queries
2. Launch the Gradio interface at `http://127.0.0.1:7900`

### Using the UI

**Input:**
- Enter your HR policy question in the textbox
- Adjust "Top-K Results" (1-15) to retrieve more or fewer chunks

**Output Tabs:**

1. **ğŸ’¡ Generated Answer** â€” The final answer from OpenAI GPT-4o-mini, grounded in retrieved policy context
2. **ğŸ“š Retrieved Context** â€” The policy excerpts that were used to generate the answer, ranked by FAISS similarity
3. **ğŸ” Retrieval Table** â€” Metadata table showing:
   - Rank: Retrieval position (1=most relevant)
   - Distance: FAISS similarity score (0-1, higher is better)
   - Source PDF: Which policy document
   - Page: Page number in the document
4. **ğŸ“Š Evaluation & Diagnostics** â€” Detailed metrics:
   - Retrieval coverage (single vs. multi-PDF)
   - Average similarity score
   - Failure warnings and diagnostics

### Example Questions

- "Is a student employee eligible for the Employee Tuition Affordability Program?"
- "What are the eligibility requirements for Family and Medical Leave?"
- "How do I apply for leave?"
- "What policies cover performance evaluations?"

---

## ğŸ—ï¸ System Architecture

```
User Query
    â†“
[FAISS Retrieval] â†’ Retrieve top-K policy chunks
    â†“
[Context Assembly] â†’ Format chunks with source/page info
    â†“
[OpenAI LLM] â†’ Generate answer grounded in context
    â†“
[Evaluation] â†’ Assess retrieval quality & answer reliability
    â†“
[Logging] â†’ Store failure events in logs/rag_failures.jsonl
```

### Key Modules

- **`src/retrieval/faiss_retriever.py`** â€” FAISS vector search, deterministic mock embeddings, retrieval diagnostics
- **`src/ui/rag_evaluation_ui.py`** â€” Gradio interface, RAG pipeline orchestration, evaluation & logging
- **`src/openai_utils.py`** â€” OpenAI API wrapper with error handling
- **`src/ingestion/pdf_ingestor.py`** â€” PDF chunking (91 chunks from 12 PDFs)
- **`src/embeddings/embedding_generator.py`** â€” Embedding generation with batch processing

### Data Files

- **`temp_storage/03_embedded_documents.json`** â€” All 91 document chunks with metadata
- **`temp_storage/04_metadata_mapping.json`** â€” Document metadata index
- **`logs/rag_failures.jsonl`** â€” Append-only log of failure events

---

## ğŸ§ª Testing

Run startup tests without launching the UI:

```bash
./.venv/bin/python3 -c "
import sys
sys.path.insert(0, '.')
from src.ui.rag_evaluation_ui import run_startup_tests
run_startup_tests()
"
```

---

## ğŸ“Š Understanding the Output

### Good Retrieval Signals
- âœ… **Multiple PDFs** â€” Queries span 2+ policy documents
- âœ… **High similarity** â€” Average distance > 0.15
- âœ… **Clear answer** â€” LLM provides specific information with citations

### Warning Signs
- âš ï¸ **Low similarity** â€” Distance < 0.15 indicates weak retrieval
- âš ï¸ **Single source** â€” All results from one PDF (narrow coverage)
- âš ï¸ **"Not found"** â€” Answer indicates information not covered in policies
- âš ï¸ **No multi-PDF** â€” Query may not have broad enough policy coverage

---

## ğŸ”§ Advanced Usage

### Custom Prompts

To modify the system prompt used for grounding the LLM, edit the `generate_rag_answer()` function in `src/ui/rag_evaluation_ui.py`.

### Failure Analysis

View logged failures:

```bash
cat logs/rag_failures.jsonl | jq .
```

Each entry includes:
- Timestamp
- Query
- Top retrieved sources
- Average similarity score
- Failure reasons

### Real Embeddings (Future)

To use real OpenAI embeddings instead of mock/deterministic embeddings:
1. Create embeddings via `src/embeddings/embedding_generator.py`
2. Store in FAISS index using `src/storage/faiss_indexer.py`
3. Update `src/retrieval/faiss_retriever.py` to load from disk index

---

## ğŸ› Troubleshooting

### "FAISS index file not found"
â†’ The system automatically builds an in-memory index from document embeddings on first run.

### Low similarity scores
â†’ This is expected with deterministic mock embeddings. Real OpenAI embeddings will improve scores significantly.

### OpenAI API errors
â†’ Check that `OPENAI_API_KEY` is set in `.env` file
â†’ Verify API quota and rate limits

### No retrieval results
â†’ Try rephrasing the query with HR policy-specific keywords
â†’ Increase Top-K to retrieve more candidates

---

## ğŸ“š Related Documentation

- [STEP 1: PDF Ingestion](STEP1_INGESTION.md)
- [STEP 2B: FAISS Indexing](STEP2B_FAISS_INDEXING.md)
- [Setup Guide](SETUP_GUIDE.md)

---

**Status**: âœ… Full RAG system working end-to-end with real OpenAI generation

Last updated: December 22, 2025
